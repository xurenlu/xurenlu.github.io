{"title":"150行代码,搞定中文分词","zzzContent":"中文分词一直是一个看起来似乎比较神秘的东西。记得java中的lucene好像自带了两个分词器。一个是按汉字分，就是一个字分成一个词。比如\"我要到饭馆吃饭“，就被分成”我/要/到/饭/馆/吃/饭\".别一个是相邻的两个字分成一个词，分出来的结果是\"我要/要到/到饭/饭馆/馆吃/吃饭\".然而这两种虽说在做搜索时建索引什么的操作时也是相当有用的，但是毕竟是一种权宜之计，咱不能一直停留在这个水平上。\n\n下面是来自<a href=\"http://rubyforge.org/projects/rlucene/\">Rlucene</a>的一段示例代码，154行，利用sogou的词库搞定了中文分词。代码如下:\n\n#! /usr/bin/ruby\nrequire \"socket\"\n#通过网络得到分词结果\ndef segChinese_net(line)\n@conn=TCPSocket.open(\"localhost\",1099)\n@conn.write(line)\nline=@conn.read\nreturn line.split(\" \")\nend\n@f= open(\"dict/sogou.txt\")\n@datas=@f.read.split(\"n\")\n@f.close\n\n@f2=open(\"dict/firstword.txt\")\n@firstwords=@f2.read.split(\"n\")\n@f2.close\n@maintable=Hash.new()\n@firstwords.each{|x|\n@maintable[x]=[x]\n}\n@datas.each{|x|\n@maintable[x[0].chr+x[1].chr+x[2].chr].push(x)\n}\ndef segChinese(line)\ntemp=0\nmax=(line.length/3)\nwords=[]\nwhile(temp\npos=temp*3\nstr1=line[pos].chr+line[pos+1].chr+line[pos+2].chr\nstr12=str1\nstr1234=str1\nif(temp&lt;(max-1))\nstr2=line[pos+3].chr+line[pos+4].chr+line[pos+5].chr\nstr12=line[pos].chr+line[pos+1].chr+line[pos+2].chr+line[pos+3].chr+line[pos+4].chr+line[pos+5].chr\nend\nif(temp&lt;(max-3))\nstr1234=line[pos].chr+line[pos+1].chr+line[pos+2].chr+line[pos+3].chr+line[pos+4].chr+line[pos+5].chr+line[pos+6].chr+line[pos+7].chr+line[pos+8].chr+line[pos+9].chr+line[pos+10].chr+line[pos+11].chr\nend\nif(!@maintable[str1].nil?)\nif(!@maintable[str1].index(str1234).nil?)\ntemp+=4\nwords.push(str1234)\nelse\nif(!@maintable[str1].index(str12).nil?)\ntemp+=2\nwords.push(str12)\nelse\nwords.push(str1)\ntemp+=1\nend\nend\nelse\nwords.push(str1)\ntemp+=1\nend\nend\nreturn words\nend\n#return an array of the segemention.\n#得到当前句子的分词结果.\ndef segment(str)\n\nchars=Hash.new(0)\nchars[\"char\"]=\"~!@#$%^&amp;*()_+}{[]\\|\"':;/?&gt;&lt;,. tnba\"\nchars[\"num\"]=\"0123456789\"\nchars[\"alpha\"]=\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\nbytes=Hash.new(0)\ni=0\nbuffer=\"\"\n#oldtype 表示上一个字符的种类:字符还是字母还是数字\n#newtype表示本字符的种类\n#type:1,表示字符,2表示数字,3表示字母,4表示属于汉字(多字节字符)\n#Buffer用来存储当未完成的一个词.\n#只有当当前字符类型与前一个字符类型不一致时才输出buffer,并清空buffer,改为当前字符值.\n#当与前一字符类型相同时，仅是把当前字符推入buffer(加到buffer的后面)\n#oldtype为4时，当输出一个buffer时,还需要对当前buffer进行中文分词\noldtype=0\nnewtype=0\nflag=0\nwords=[]\n0.upto(str.length-1){|a|\nif(flag&gt;0)\nflag=flag-1\nelse\nx=str[a]\nif(!chars[\"char\"].index(x.chr).nil?)\nnewtype=1\nif(newtype==oldtype)\nbuffer=buffer+x.chr\nelse\nif(oldtype==4)\nwords.concat(segChinese(buffer.strip))\nbuffer=x.chr\nelse\nwords.push( buffer) if (buffer.strip!=\"\")\nbuffer=x.chr\nend\nend\noldtype=1\nend\nif(!chars[\"num\"].index(x.chr).nil?)\nnewtype=2\nif(newtype==oldtype)\nbuffer=buffer+x.chr\nelse\nif(oldtype==4)\nwords.concat(segChinese(buffer.strip))\nbuffer=x.chr\nelse\nwords.push(buffer) if (buffer.strip!=\"\")\nbuffer=x.chr\nend\nend\noldtype=2\nend\nif(!chars[\"alpha\"].index(x.chr).nil?)\nnewtype=3\nif(newtype==oldtype)\nbuffer=buffer+x.chr\nelse\nif(oldtype==4)\nwords.concat(segChinese(buffer.strip))\nbuffer=x.chr\nelse\nwords.push(buffer) if (buffer.strip!=\"\")\nbuffer=x.chr\nend\nend\noldtype=3\nend\nif(x&gt;127)\nflag=+2\nnewtype=4\nif(newtype==oldtype)\nbuffer=buffer+str[a].chr+str[a+1].chr+str[a+2].chr\nelse\nwords.push(buffer) if(buffer.strip!=\"\")\nbuffer=str[a].chr+str[a+1].chr+str[a+2].chr\nend\noldtype=4\nend\nend\n}\nif(oldtype==4)\nwords.concat(segChinese(buffer.strip))\nelse\nwords.push(buffer) if(buffer.strip!=\"\")\nend\ntotal=\"\"\nwords.each{|x| total=total+x+\" \"}\nreturn total\n#puts total\nend\n\nruby果然就是牛B极了。在学着用BCB折腾nsapi的时候 ，发现了asp,于是我觉得asp真是一个比较牛B的东西。接着发现了php,于是我扔掉了asp, 接着发现python比php有前途，现在，python还没摸透，又瞅 上ruby了。世界变化真是太快了，我快跟不上啦。\n\n不过要实用，可能还得做些调整。 一般来说，分词的词库得自己整，sogou这个示例可以，真起来肯定有问题。而且要实用，在结果准确度上还要下功能。","postDate":"2007-06-16 23:50:01","postId":411,"type":"post","status":"publish","imported":true,"file":"411.md"}