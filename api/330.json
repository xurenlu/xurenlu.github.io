{"title":"用python开发SeachEngin spiders的构想:","zzzContent":"<p>用python开发SeachEngin spiders的构想: </p> <p>1.首先，spiders应该有一个控制中心。 </p> <p>２.每一个spider占用的资源都是有限的，并且是不太可靠的。如果哪个spider不能胜任，它也不会影响整个系统的运行。 </p> <p>3.spider们的任务也是各自分开的，分为以下几种: </p> <blockquote dir=ltr> <p>读取WWW文件，储存内容。 </p> <p>分析html文件，获取它包含的链接。一般每个网页都能找出不少链接。</p> <p>读取html内容，进行过滤处理。主要是把垃圾信息丢掉，比如HTML标签，为作弊而设置的一些内容。</p> <p>对\"净化\"后的文件分段，分句,分词，索引,存入索引库.</p> <p> </p> <p> </p></blockquote> <p>4.读得的内容要进行分析,其中最难的部分是一个分析过程，比如去除无关信息，分词处理等.</p> <p>5.分词可以自定义接口，然后做成一个factory模式，可以加载不同的分词器。比如上次做过的，简单分词法:英文按空格来分词，中文则机械地按四个字节(gbk码)或6个字节(UTF-8编码)来分词，这样每两个汉字构成一个词。这个缺点是分词准确率相当低，但是，在频率统计上，相似度分析上是有一定用处的。</p> <p> </p>","postDate":"2006-12-14 21:17:14","postId":330,"type":"post","status":"publish","imported":true,"file":"330.md"}